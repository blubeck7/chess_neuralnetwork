{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load packages\n",
    "import sys\n",
    "sys.path.append(\"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/\")\n",
    "import boto3\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack as vstack\n",
    "import chess, chess.pgn\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gzip\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "BOARD_LENGTH = 768 #chess board is 8 x 8 and 12 different pieces\n",
    "\n",
    "## Vector representation of chess board\n",
    "# v = 1 x BOARD_LENGTH\n",
    "#\n",
    "# White = Upper Case, black = lower case\n",
    "# Piece order: P, N, B, R, Q, K, p, n, b, r, q, k\n",
    "# Board order:\n",
    "#    Start at square a1. Move across the columns to square h1.\n",
    "#    Then go up a row to square a2. Move across the columns to square h2.\n",
    "#    Repeat until square h8\n",
    "#    i.e. 0 - a1, 1 - b1, ..., 7 - h1, 8 - a2, ..., 63 - h8\n",
    "#\n",
    "# Board vector indices: \n",
    "# v[0,...,63] = P, v[64,...,127] = N, ..., v[704,...,767] = k\n",
    "# v[0,...,7] = row 1; v[8,...,15] = row 2, ..., v[56,...,63] = row 8\n",
    "# v[0] = col a, v[1] = col b, ..., v[7] = col h\n",
    "\n",
    "PIECE_OFFSETS = {'P': 0, 'N': 64, 'B': 128, 'R': 192, 'Q': 256, 'K': 320,\n",
    "                 'p': 384, 'n': 448, 'b': 512, 'r': 576, 'q': 640, 'k': 704}\n",
    "\n",
    "RESULTS_DICT = {'1-0': 1,'1/2-1/2': 0,'0-1': -1}\n",
    "RESULTS_LIST = [1, 0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "\n",
    "In this section, we build, train and test a feedforward neural network on a small sample games to determine an appropriate neural network architecture. We use tensorflow to train the neural network. Then we use AWS to scale up the neural network and train it using all of the data. In the parse data program, the cleaned data was split up into smaller files so that the files can be processed by different cores. We use distributed tensorflow to parallelize the training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Built on Sample Data\n",
    "\n",
    "We build a neural net on a sample of data in order to determine a resonable number of hidden nodes to use to\n",
    "train the full model. We use a sample of data so that the training times are short enough that the model can be iterated on. AWS will then be used to train the model on the data from 2007 to 2016, roughly 10M games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load 2006 and 2007 data. train/test split. save using numpy/scipy functions\n",
    "# stats about the data - Number of games\n",
    "#   Shortest game, average game, longest game in terms of # of moves\n",
    "#   difference in ratings, min, ave, max\n",
    "#   num of white wins, draws and loses\n",
    "#   white win % as a func of rating diff\n",
    "# code to train nn\n",
    "# bias variance curves and cost vs iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "In this section, we load the data. Then we check that it is accurate. Then we split the data into a training (80%) dataset and a test (20%) dataset. Finally, we save the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2007_stats.pickle',\n",
       " 'ficsgamesdb_2006_chess2000_nomovetimes_1519260_1_mat.pickle.gz',\n",
       " '.DS_Store',\n",
       " 'ficsgamesdb_2006_chess2000_nomovetimes_1519260_2_mat.pickle.gz',\n",
       " 'ficsgamesdb_2006_chess2000_nomovetimes_1519260_2_stats.pickle',\n",
       " 'ficsgamesdb_2006_chess2000_nomovetimes_1519260_3_stats.pickle',\n",
       " 'xTest.npz',\n",
       " 'ficsgamesdb_2006_chess2000_nomovetimes_1519260_1_stats.pickle',\n",
       " 'ficsgamesdb_2006_chess2000_nomovetimes_1519260_0_stats.pickle',\n",
       " 'yTest.npz',\n",
       " 'yTrain.npz',\n",
       " 'xTrain.npz',\n",
       " 'ficsgamesdb_2006_chess2000_nomovetimes_1519260_3_mat.pickle.gz',\n",
       " 'ficsgamesdb_2006_chess2000_nomovetimes_1519260_0_mat.pickle.gz',\n",
       " '2007_mat.pickle.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# directory with data processed from pgn files\n",
    "dataDir = '../data/clean'\n",
    "dataFiles = os.listdir(dataDir)\n",
    "dataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data into memory\n",
    "boardList = []\n",
    "resultList = []\n",
    "statsList = []\n",
    "\n",
    "for dataFile in dataFiles:\n",
    "    if dataFile.endswith('.gz'):\n",
    "        with gzip.open(os.path.join(dataDir,dataFile), 'rb') as f:\n",
    "            pickledList = f.read()\n",
    "            boardMat, resultMat = pickle.loads(pickledList) #matrix of board positions, matrix of game results\n",
    "            boardList.append(boardMat)\n",
    "            resultList.append(resultMat)    \n",
    "    \n",
    "    elif dataFile.endswith('.pickle'):\n",
    "        stats = pd.read_pickle(os.path.join(dataDir,dataFile))\n",
    "        statsList.append(stats)\n",
    "        \n",
    "#Data check that results vector and board matrix are in sync\n",
    "\n",
    "s = 0\n",
    "for i in range(len(boardList)):\n",
    "    s = s + np.sum(boardList[i].shape[0] == resultList[i].shape[0])\n",
    "print(len(boardList) - s)\n",
    "#if not zero then there is a problem with the parsing of the data.\n",
    "#need to look into parse data code and raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine list of matrices and into one matrix\n",
    "\n",
    "boards = scipy.sparse.vstack(boardList)\n",
    "results = np.concatenate(resultList)\n",
    "stats = pd.concat(statsList)\n",
    "\n",
    "del [boardList, resultList, statsList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# memory usage in GiB\n",
    "dataGiB = boards.nnz * boards.dtype.itemsize / (1024**3)\n",
    "colGiB = boards.indices.shape[0] * boards.indices.dtype.itemsize / (1024**3)\n",
    "rowGiB = boards.indptr.shape[0] * boards.indptr.dtype.itemsize / (1024**3)\n",
    "print('Boards total GiB: ' + str(dataGiB + colGiB + rowGiB))\n",
    "\n",
    "dataGiB = results.shape[0] * results.dtype.itemsize / (1024**3)\n",
    "print('Results total GiB: ' + str(dataGiB))\n",
    "\n",
    "print('Number of Board Positions: ' + str(boards.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(boards, results, train_size = 0.8)\n",
    "\n",
    "del [boards, results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save train and test split\n",
    "# scipy.sparse.save_npz(os.path.join(dataDir, 'xTrain.npz'), xTrain, compressed=True)\n",
    "# scipy.sparse.save_npz(os.path.join(dataDir, 'xTest.npz'), xTest, compressed=True)\n",
    "# np.savez_compressed(os.path.join(dataDir, 'yTrain.npz'), yTrain)\n",
    "# np.savez_compressed(os.path.join(dataDir, 'yTest.npz'), yTest)\n",
    "xTrain = scipy.sparse.load_npz(os.path.join(dataDir, 'xTrain.npz'))\n",
    "yTrain = np.load(os.path.join(dataDir, 'yTrain.npz'))['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes = RESULTS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [-1],\n",
       "       [ 1],\n",
       "       ..., \n",
       "       [-1],\n",
       "       [ 1],\n",
       "       [-1]], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Training Code\n",
    "\n",
    "In this section, we implement a feed forward neural network with one hidden layer. The hidden layer uses the rectified linear function, and the output layer uses the softmax function. Cross-entropy (multinomial likelihood) is used as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "MINIBATCHSIZE = 512\n",
    "#MAXITER = 2000\n",
    "#RELTOL = 0.00001\n",
    "#MAXEPOCHS = 2\n",
    "#logs_path = '../log'\n",
    "#LAMBDA = 1 # strength of L2 regularization\n",
    "\n",
    "# layer parameters\n",
    "numInputNodes = BOARD_LENGTH\n",
    "#numHiddenNodes = 100\n",
    "#numOutputNodes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BoardFunction:\n",
    "    \n",
    "    def __init__(self, numInputNodes, numHiddenNodes, numOutputNodes):\n",
    "        # layers\n",
    "        self.numInputNodes = numInputNodes\n",
    "        self.numHiddenNodes = numHiddenNodes\n",
    "        self.numOutputNodes = numOutputNodes\n",
    "        \n",
    "        # weight matrices\n",
    "        self.hiddenWeights = np.empty((self.numInputNodes, self.numHiddenNodes), dtype = np.float_)\n",
    "        self.hiddenBiases = np.empty((1, self.numHiddenNodes), dtype = np.float_)\n",
    "        self.outputWeights = np.empty((self.numHiddenNodes, self.numOutputNodes), dtype = np.float_)\n",
    "        self.outputBiases = np.empty((1, self.numOutputNodes), dtype = np.float_)\n",
    "        \n",
    "    def initWeights(self):\n",
    "        '''\n",
    "        Randomly initializes the weight matrices\n",
    "        '''\n",
    "        \n",
    "        self.hiddenWeights = np.random.normal(size = self.hiddenWeights.shape)\n",
    "        self.hiddenBiases = np.random.normal(size = self.hiddenBiases.shape)\n",
    "        self.outputWeights = np.random.normal(size = self.outputWeights.shape)\n",
    "        self.outputBiases = np.random.normal(size = self.outputBiases.shape)\n",
    "        \n",
    "    def _relu(self, X):\n",
    "        '''\n",
    "        X - matrix\n",
    "        \n",
    "        returns element wise max of X and zero\n",
    "        '''\n",
    "        \n",
    "        return(np.maximum(X,0))\n",
    "    \n",
    "    def _softmax(self, X):\n",
    "        shiftX = X - np.amax(X, axis = 1, keepdims = True)\n",
    "        exps = np.exp(shiftX)\n",
    "        sums = np.sum(exps, axis = 1, keepdims = True)\n",
    "        \n",
    "        return(exps / sums)\n",
    "    \n",
    "    def predict(self, board):\n",
    "        '''\n",
    "        board - csr matrix: sparse row matrix of encoded board positions\n",
    "    \n",
    "        returns probs - numpy array: a matrix containing the probability of a win, draw or loss\n",
    "        '''\n",
    "        \n",
    "        numBoards = board.shape[0]\n",
    "        \n",
    "        hiddenWeights = board.dot(self.hiddenWeights)\n",
    "        hiddenBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.hiddenBiases)\n",
    "        hiddenIn = hiddenWeights + hiddenBiases\n",
    "        hiddenOut = self._relu(hiddenIn) #rectified linear element-wise max with zero\n",
    "        \n",
    "        outputWeights = hiddenOut.dot(self.outputWeights)\n",
    "        outputBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.outputBiases)\n",
    "        outputIn = outputWeights + outputBiases\n",
    "        outputOut = self._softmax(outputIn)\n",
    "        \n",
    "        minProb = np.finfo(np.float64).tiny # avoid numerical issues with zero probs\n",
    "        \n",
    "        return(np.maximum(outputOut, minProb))\n",
    "    \n",
    "    def loss(self, board, result):\n",
    "        '''\n",
    "        board - csr matrix: sparse row matrix of encoded board positions\n",
    "        result - 1d array: one hot enconding of result\n",
    "        \n",
    "        returns the cross entropy (multinomial log-likelihood) for the sample\n",
    "        '''\n",
    "        \n",
    "        probs = self.predict(board)\n",
    "        aveLogLikelihood = -np.sum(result * np.log(probs)) / board.shape[0]\n",
    "        \n",
    "        return(aveLogLikelihood)\n",
    "    \n",
    "    def calcGradients(self, board, result):\n",
    "        '''\n",
    "        board - csr matrix: sparse row matrix of encoded board positions\n",
    "        result - 1d array: one hot enconding of result\n",
    "        \n",
    "        J = cross entropy loss function\n",
    "        '''\n",
    "        \n",
    "        numBoards = board.shape[0]\n",
    "        \n",
    "        # feed forward\n",
    "        hiddenWeights = board.dot(self.hiddenWeights)\n",
    "        hiddenBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.hiddenBiases)\n",
    "        hiddenIn = hiddenWeights + hiddenBiases\n",
    "        hiddenOut = self._relu(hiddenIn) #rectified linear element-wise max with zero\n",
    "        \n",
    "        outputWeights = hiddenOut.dot(self.outputWeights)\n",
    "        outputBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.outputBiases)\n",
    "        outputIn = outputWeights + outputBiases\n",
    "        outputOut = self._softmax(outputIn)\n",
    "        \n",
    "        # compute gradients\n",
    "        d1 = outputOut - result\n",
    "        d2 = d1.dot(self.outputWeights.transpose()) * np.sign(hiddenOut)\n",
    "        \n",
    "        # D J(outputWeights)\n",
    "        DJoutW = hiddenOut.transpose().dot(d1) / numBoards\n",
    "        \n",
    "        # D J(outputBiases)\n",
    "        DJoutB = np.sum(d1.dot(np.eye(result.shape[1])), axis = 0) / numBoards\n",
    "        \n",
    "        # D J(hiddenWeights)\n",
    "        DJhidW = board.transpose().dot(d2) / numBoards\n",
    "        \n",
    "        # D J(hiddenBiases)\n",
    "        DJhidB = np.sum(d2, axis = 0) / numBoards\n",
    "        \n",
    "        return(DJoutW, DJoutB, DJhidW, DJhidB)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1, m2,m3,m4 = f.hiddenWeights, f.hiddenBiases, f.outputWeights, f.outputBiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = BoardFunction(BOARD_LENGTH, 3000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.hiddenWeights, f.hiddenBiases, f.outputWeights, f.outputBiases = m1,m2,m3,m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.initWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testVec = xTrain[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = mlb.fit_transform(yTrain[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float64, shape = [None, f.numInputNodes])\n",
    "y = tf.placeholder(tf.float64, shape = [None, f.numOutputNodes])\n",
    "\n",
    "# layer weights and biases\n",
    "hiddenWeights = tf.Variable(f.hiddenWeights, dtype = tf.float64)\n",
    "hiddenBiases = tf.Variable(f.hiddenBiases, dtype = tf.float64)\n",
    "outputWeights = tf.Variable(f.outputWeights, dtype = tf.float64)\n",
    "outputBiases = tf.Variable(f.outputBiases, dtype = tf.float64)\n",
    "\n",
    "# computations\n",
    "hidden = tf.nn.relu(tf.add(tf.matmul(x, hiddenWeights), hiddenBiases))\n",
    "output = tf.add(tf.matmul(hidden, outputWeights), outputBiases)\n",
    "\n",
    "# cost function\n",
    "pred = tf.nn.softmax(logits = output)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = output))\n",
    "\n",
    "# train\n",
    "trainStep = tf.train.GradientDescentOptimizer(0.5).compute_gradients(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as s:\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    b, c, d = s.run([pred, cost, trainStep], feed_dict = {x: testVec.toarray(), y: r})\n",
    "    s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.357267404708651"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.loss(testVec,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+000,   5.49102659e-204,   5.13448403e-145],\n",
       "       [  1.00000000e+000,   3.01993258e-265,   8.90155037e-082],\n",
       "       [  1.00000000e+000,   1.07235943e-216,   4.32219641e-158],\n",
       "       [  1.00000000e+000,   4.51320738e-091,   9.65908182e-070],\n",
       "       [  9.99483101e-001,   2.69832121e-086,   5.16899005e-004],\n",
       "       [  1.00000000e+000,   2.78646300e-151,   3.62220323e-024],\n",
       "       [  1.00000000e+000,   1.27503933e-192,   3.91854584e-125],\n",
       "       [  1.00000000e+000,   2.87236457e-062,   2.25777730e-071],\n",
       "       [  4.51945766e-003,   1.11887908e-028,   9.95480542e-001],\n",
       "       [  1.00000000e+000,   5.72030266e-144,   1.46704777e-070]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.predict(testVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DoutW, DoutB, DhidW, DhidB = f.calcGradients(testVec,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40409952, -0.16742241, -0.18327511, ...,  0.10448689,\n",
       "        0.01214752, -0.67927708])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DhidB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40409952, -0.16742241, -0.18327511, ...,  0.10448689,\n",
       "         0.01214752, -0.67927708]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[1][0] -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57798709152e-14\n",
      "-8.61834000975e-16\n",
      "-5.98323253871e-14\n",
      "-5.55111512313e-17\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(d[0][0] - DhidW))\n",
    "print(np.sum(d[1][0] - DhidB))\n",
    "print(np.sum(d[2][0] - DoutW))\n",
    "print(np.sum(d[3][0] - DoutB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        , -0.05580747,  0.        , ...,  0.        ,\n",
       "         0.        , -0.11405194],\n",
       "       [-0.06951448,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.13577456, -0.16827343],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.predict(testVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTrain[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.loss(testVec,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.hiddenWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load sample data\n",
    "dataDir = '../sample/clean'\n",
    "dataFiles = os.listdir(dataDir)\n",
    "\n",
    "boardList = []\n",
    "resultList = []\n",
    "statsList = []\n",
    "for dataFile in dataFiles:\n",
    "    if dataFile.endswith('.gz'):\n",
    "        with gzip.open(os.path.join(dataDir,dataFile), 'rb') as f:\n",
    "            pickledList = f.read()\n",
    "            boardMat, resultMat = pickle.loads(pickledList) #matrix of board positions, matrix of game results\n",
    "            boardList.append(boardMat)\n",
    "            resultList.append(resultMat)    \n",
    "    \n",
    "    elif dataFile.endswith('.pickle'):\n",
    "        stats = pd.read_pickle(os.path.join(dataDir,dataFile))\n",
    "        statsList.append(stats)\n",
    "        \n",
    "#Data check that results vector and board matrix are in sync\n",
    "\n",
    "s = 0\n",
    "for i in range(len(boardList)):\n",
    "    s = s + np.sum(boardList[i].shape[0] == resultList[i].shape[0])\n",
    "print(len(boardList) - s)\n",
    "#if not zero then there is a problem with the parsing of the data.\n",
    "#need to look into parse data code and raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boards = vstack(boardList)\n",
    "results = np.concatenate(resultList)\n",
    "stats = pd.concat(statsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(boards.shape)\n",
    "print(results.shape)\n",
    "print(stats.Moves.sum())\n",
    "print(stats.shape[0])\n",
    "print(stats.Moves.sum() / stats.shape[0] / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boards.getnnz() * 8 * 3 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes = [-1,0,1])\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(boards, results, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generator to feed batches to model\n",
    "def nextBatch(totObs, batchSize):\n",
    "    # initialize\n",
    "    numBatches = totObs // batchSize\n",
    "    tail = totObs % batchSize\n",
    "    batch = 0\n",
    "    \n",
    "    while True:\n",
    "        batch = (batch + 1) % numBatches\n",
    "        if batch == 1:\n",
    "            firstInd = 0\n",
    "            lastInd = batchSize\n",
    "        elif batch == 0:\n",
    "            firstInd = lastInd\n",
    "            lastInd = lastInd + batchSize + tail\n",
    "        else:\n",
    "            firstInd = lastInd\n",
    "            lastInd = lastInd + batchSize\n",
    "        \n",
    "        yield firstInd, lastInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "MINIBATCH_SIZE = 512\n",
    "MAXITER = 2000\n",
    "RELTOL = 0.00001\n",
    "MAXEPOCHS = 2\n",
    "logs_path = '../log'\n",
    "#LAMBDA = 1 # strength of L2 regularization\n",
    "\n",
    "# layer parameters\n",
    "numInputNodes = BOARD_LENGTH\n",
    "numHiddenNodes = 100\n",
    "numOutputNodes = 3\n",
    "\n",
    "# input and output placeholders\n",
    "#x = tf.sparse_placeholder(tf.float64, shape = [None, numInputNodes])\n",
    "x = tf.placeholder(tf.float64, shape = [None, numInputNodes])\n",
    "y = tf.placeholder(tf.float64, shape = [None, numOutputNodes])\n",
    "\n",
    "# layer weights and biases\n",
    "hiddenWeights = tf.Variable(tf.random_normal([numInputNodes, numHiddenNodes], 0, 1, dtype = tf.float64))\n",
    "hiddenBiases = tf.Variable(tf.random_normal([numHiddenNodes], 0, 1, dtype = tf.float64))\n",
    "outputWeights = tf.Variable(tf.random_normal([numHiddenNodes, numOutputNodes], 0, 1, dtype = tf.float64))\n",
    "outputBiases = tf.Variable(tf.random_normal([numOutputNodes], 0, 1, dtype = tf.float64))\n",
    "\n",
    "# computations\n",
    "# hidden = tf.nn.relu(tf.add(tf.sparse_tensor_dense_matmul(x, hiddenWeights), hiddenBiases))\n",
    "hidden = tf.nn.relu(tf.add(tf.matmul(x, hiddenWeights), hiddenBiases))\n",
    "output = tf.add(tf.matmul(hidden, outputWeights), outputBiases)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = output))\n",
    "\n",
    "# add L2 regularization\n",
    "# costReg = cost + lambda_ * (tf.nn.l2_loss(hiddenWeights) + tf.nn.l2_loss(outputWeights)))\n",
    "\n",
    "# optimization method to minimize cost function\n",
    "trainStep = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "\n",
    "# summary statistics to save\n",
    "tf.summary.scalar('Cost', cost)\n",
    "#summary_op = tf.merge_all_summaries()\n",
    "\n",
    "# train model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batchFeeder = nextBatch(xTrain.shape[0], MINIBATCH_SIZE)\n",
    "    numIter = 0\n",
    "    currCost = 0\n",
    "    prevCost = 0\n",
    "    relImp = 1\n",
    "    \n",
    "    # create log writer object\n",
    "    #writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "       \n",
    "    \n",
    "    while numIter < MAXITER and relImp > RELTOL:\n",
    "        # update number of iterations\n",
    "        numIter = numIter + 1\n",
    "        \n",
    "        # get next batch\n",
    "        firstInd, lastInd = next(batchFeeder)\n",
    "        #xTrain_ = xTrain[firstInd:lastInd]\n",
    "        xVal = xTrain[firstInd:lastInd].toarray()\n",
    "        \n",
    "        # convert sparse xTrain matrix to tensorflow sparse value\n",
    "        #xTrain_ = xTrain_.tocoo()\n",
    "        #xVal = tf.SparseTensorValue(\n",
    "        #    indices = np.stack((xTrain_.row, xTrain_.col), axis = -1),\n",
    "        #    values = xTrain_.data,\n",
    "        #    dense_shape = [xTrain_.shape[0], xTrain_.shape[1]])\n",
    "        #xVal = tf.sparse_reorder(xVal)\n",
    "    \n",
    "        # one-hot encoding for yTrain\n",
    "        yVal = mlb.fit_transform(yTrain[firstInd:lastInd])\n",
    "    \n",
    "        prevCost = currCost\n",
    "        _ , currCost = sess.run([trainStep, cost], feed_dict = {x: xVal, y: yVal})\n",
    "        \n",
    "        # write log\n",
    "        # writer.add_summary(summary, epoch * batch_count + i)\n",
    "        \n",
    "        if numIter % 50 == 0:\n",
    "            print('Iteration: {0} Cost: {1}'.format(numIter, currCost))\n",
    "        \n",
    "        if prevCost !=0:\n",
    "            relImp = abs((prevCost - currCost) / prevCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prevCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "write tensorflow code, add tensorboard for visualizations, then distributed tensorflow\n",
    "go through amazon sage maker doc and try to run parse data code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for Report\n",
    "\n",
    "From Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. https://arxiv.org/abs/1609.04836 :\n",
    "\n",
    "The stochastic gradient descent method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, usually 32--512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = chess.Board(fen='8/5r1k/2R4P/3p1Pb1/2pP4/8/P4RBK/2r5 b - - 6 42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.show_config()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
