{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack as vstack\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get files from s3\n",
    "# s3_client = boto3.client('s3')\n",
    "# s3_client.download_file('brianlubeck2', 'chess/data/clean/xTrain0.npz', '/home/ec2-user/chess/data/clean/xTrain0.npz')\n",
    "# s3_client.download_file('brianlubeck2', 'chess/data/clean/xTrain1.npz', '/home/ec2-user/chess/data/clean/xTrain1.npz')\n",
    "# s3_client.download_file('brianlubeck2', 'chess/data/clean/xTrain2.npz', '/home/ec2-user/chess/data/clean/xTrain2.npz')\n",
    "# s3_client.download_file('brianlubeck2', 'chess/data/clean/xTrain3.npz', '/home/ec2-user/chess/data/clean/xTrain3.npz')\n",
    "# s3_client.download_file('brianlubeck2', 'chess/data/clean/yTrain0.npz', '/home/ec2-user/chess/data/clean/yTrain0.npz')\n",
    "# s3_client.download_file('brianlubeck2', 'chess/data/clean/yTrain1.npz', '/home/ec2-user/chess/data/clean/yTrain1.npz')\n",
    "# s3_client.download_file('brianlubeck2', 'chess/data/clean/yTrain2.npz', '/home/ec2-user/chess/data/clean/yTrain2.npz')\n",
    "# s3_client.download_file('brianlubeck2', 'chess/data/clean/yTrain3.npz', '/home/ec2-user/chess/data/clean/yTrain3.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "BOARD_LENGTH = 768 #chess board is 8 x 8 and 12 different pieces\n",
    "\n",
    "## Vector representation of chess board\n",
    "# v = 1 x BOARD_LENGTH\n",
    "#\n",
    "# White = Upper Case, black = lower case\n",
    "# Piece order: P, N, B, R, Q, K, p, n, b, r, q, k\n",
    "# Board order:\n",
    "#    Start at square a1. Move across the columns to square h1.\n",
    "#    Then go up a row to square a2. Move across the columns to square h2.\n",
    "#    Repeat until square h8\n",
    "#    i.e. 0 - a1, 1 - b1, ..., 7 - h1, 8 - a2, ..., 63 - h8\n",
    "#\n",
    "# Board vector indices: \n",
    "# v[0,...,63] = P, v[64,...,127] = N, ..., v[704,...,767] = k\n",
    "# v[0,...,7] = row 1; v[8,...,15] = row 2, ..., v[56,...,63] = row 8\n",
    "# v[0] = col a, v[1] = col b, ..., v[7] = col h\n",
    "\n",
    "PIECE_OFFSETS = {'P': 0, 'N': 64, 'B': 128, 'R': 192, 'Q': 256, 'K': 320,\n",
    "                 'p': 384, 'n': 448, 'b': 512, 'r': 576, 'q': 640, 'k': 704}\n",
    "\n",
    "RESULTS_DICT = {'1-0': 1,'1/2-1/2': 0,'0-1': -1}\n",
    "RESULTS_LIST = [1, 0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "\n",
    "In this section, we build, train and test a feedforward neural network that calculates the probability a board position results in a win for white. We first use a small sample of games to determine a reasonable number of hidden nodes fro the neural network. The program ParseData.py converts the pgn data into binary vectors and saves the output. The program PrepareTrainTest.py takes the binary vectors and creates four training datasets and one testing dataset. We split the data up so that the data could be processed by different cores using mini-batch parallelism. We compare the time to train the neural network of our implementation of mini-batch parallelism versus none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Class\n",
    "\n",
    "The code uses sparse matrices to siginificantly decrease the amount of memory and instructions needed on a CPU to train the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoardFunction:\n",
    "    \n",
    "    def __init__(self, numInputNodes, numHiddenNodes, numOutputNodes, maxIter, maxEpochs):\n",
    "        # layers\n",
    "        self.numInputNodes = numInputNodes\n",
    "        self.numHiddenNodes = numHiddenNodes\n",
    "        self.numOutputNodes = numOutputNodes\n",
    "        \n",
    "        # weight matrices\n",
    "        self.hiddenWeights = np.empty((self.numInputNodes, self.numHiddenNodes), dtype = np.float_)\n",
    "        self.hiddenBiases = np.empty((1, self.numHiddenNodes), dtype = np.float_)\n",
    "        self.outputWeights = np.empty((self.numHiddenNodes, self.numOutputNodes), dtype = np.float_)\n",
    "        self.outputBiases = np.empty((1, self.numOutputNodes), dtype = np.float_)\n",
    "        \n",
    "        # learning parameters\n",
    "        self.learningRate = 0.1\n",
    "        self.minRate = 0.00001\n",
    "        self.miniBatchSize = 512\n",
    "        self.maxIter = maxIter\n",
    "        self.maxEpochs = maxEpochs\n",
    "        self.minTol = 10**(-7)\n",
    "        self.decay = True # if true decreses the learning rate if loss plateaus\n",
    "        self.logPath = '../log'\n",
    "        \n",
    "    def initWeights(self, seed = None):\n",
    "        '''\n",
    "        Randomly initializes the weight matrices\n",
    "        '''\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        self.hiddenWeights = np.random.normal(size = self.hiddenWeights.shape)\n",
    "        self.hiddenBiases = np.random.normal(size = self.hiddenBiases.shape)\n",
    "        self.outputWeights = np.random.normal(size = self.outputWeights.shape)\n",
    "        self.outputBiases = np.random.normal(size = self.outputBiases.shape)\n",
    "        \n",
    "    def _relu(self, X):\n",
    "        '''\n",
    "        X - matrix\n",
    "        \n",
    "        returns element wise max of X and zero\n",
    "        '''\n",
    "        \n",
    "        return(np.maximum(X,0))\n",
    "    \n",
    "    def _softmax(self, X):\n",
    "        shiftX = X - np.amax(X, axis = 1, keepdims = True)\n",
    "        exps = np.exp(shiftX)\n",
    "        sums = np.sum(exps, axis = 1, keepdims = True)\n",
    "        \n",
    "        return(exps / sums)\n",
    "    \n",
    "    def predict(self, board):\n",
    "        '''\n",
    "        board - csr matrix: sparse row matrix of encoded board positions\n",
    "    \n",
    "        returns probs - numpy array: a matrix containing the probability of a win, draw or loss\n",
    "        '''\n",
    "        \n",
    "        numBoards = board.shape[0]\n",
    "        \n",
    "        hiddenWeights = board.dot(self.hiddenWeights)\n",
    "        hiddenBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.hiddenBiases)\n",
    "        hiddenIn = hiddenWeights + hiddenBiases\n",
    "        hiddenOut = self._relu(hiddenIn) #rectified linear element-wise max with zero\n",
    "        \n",
    "        outputWeights = hiddenOut.dot(self.outputWeights)\n",
    "        outputBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.outputBiases)\n",
    "        outputIn = outputWeights + outputBiases\n",
    "        outputOut = self._softmax(outputIn)\n",
    "        \n",
    "        minProb = np.finfo(np.float64).tiny # avoid numerical issues with zero probs\n",
    "        \n",
    "        return(np.maximum(outputOut, minProb))\n",
    "    \n",
    "    def loss(self, board, result):\n",
    "        '''\n",
    "        board - csr matrix: sparse row matrix of encoded board positions\n",
    "        result - 1d array: one hot enconding of result\n",
    "        \n",
    "        returns the cross entropy (multinomial log-likelihood) for the sample\n",
    "        '''\n",
    "        \n",
    "        probs = self.predict(board)\n",
    "        aveLogLikelihood = -np.sum(result * np.log(probs)) / board.shape[0]\n",
    "        \n",
    "        return(aveLogLikelihood)\n",
    "    \n",
    "    def calcGradients(self, board, result):\n",
    "        '''\n",
    "        board - csr matrix: sparse row matrix of encoded board positions\n",
    "        result - 1d array: one hot enconding of result\n",
    "        \n",
    "        J = cross entropy loss function\n",
    "        '''\n",
    "        \n",
    "        numBoards = board.shape[0]\n",
    "        \n",
    "        # feed forward\n",
    "        hiddenWeights = board.dot(self.hiddenWeights)\n",
    "        hiddenBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.hiddenBiases)\n",
    "        hiddenIn = hiddenWeights + hiddenBiases\n",
    "        hiddenOut = self._relu(hiddenIn) #rectified linear element-wise max with zero\n",
    "        \n",
    "        outputWeights = hiddenOut.dot(self.outputWeights)\n",
    "        outputBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.outputBiases)\n",
    "        outputIn = outputWeights + outputBiases\n",
    "        outputOut = self._softmax(outputIn)\n",
    "        \n",
    "        # compute gradients\n",
    "        d1 = outputOut - result\n",
    "        d2 = d1.dot(self.outputWeights.transpose()) * np.sign(hiddenOut)\n",
    "        \n",
    "        # D J(outputWeights)\n",
    "        DJoutW = hiddenOut.transpose().dot(d1) / numBoards\n",
    "        \n",
    "        # D J(outputBiases)\n",
    "        DJoutB = np.sum(d1.dot(np.eye(result.shape[1])), axis = 0) / numBoards\n",
    "        \n",
    "        # D J(hiddenWeights)\n",
    "        DJhidW = board.transpose().dot(d2) / numBoards\n",
    "        \n",
    "        # D J(hiddenBiases)\n",
    "        DJhidB = np.sum(d2, axis = 0) / numBoards\n",
    "        \n",
    "        return(DJoutW, DJoutB, DJhidW, DJhidB)\n",
    "    \n",
    "    def saveIter(self, fileName, fileNum, numEpoch, numIter, loss):\n",
    "        line = str(fileNum) + ',' + str(numEpoch) + ',' + str(numIter) + ',' + str(loss) + '\\n'\n",
    "        with open(fileName, mode ='a') as f:\n",
    "            f.write(line)\n",
    "    \n",
    "    def saveWeights(self, fileName):\n",
    "        print('Saving weights to ' + fileName)\n",
    "        np.savez_compressed(fileName,\n",
    "                            hiddenWeights = self.hiddenWeights,\n",
    "                            hiddenBiases = self.hiddenBiases,\n",
    "                            outputWeights = self.outputWeights,\n",
    "                            outputBiases = self.outputBiases)\n",
    "        print('Done saving weights')\n",
    "        \n",
    "    def applyGradients(self, DJoutW, DJoutB, DJhidW, DJhidB):\n",
    "        '''\n",
    "        \n",
    "        updates each weight matrix by subtracting off the learning rate times the gradient\n",
    "        '''\n",
    "        \n",
    "        self.hiddenWeights = self.hiddenWeights - self.learningRate * DJhidW\n",
    "        self.hiddenBiases = self.hiddenBiases - self.learningRate * DJhidB\n",
    "        self.outputWeights = self.outputWeights - self.learningRate * DJoutW\n",
    "        self.outputBiases = self.outputBiases - self.learningRate * DJoutB\n",
    "    \n",
    "    def train(self, xTrain, yTrain, logFile, fileNum):\n",
    "        \n",
    "        # setup training\n",
    "        resultOneHot = MultiLabelBinarizer(classes = RESULTS_LIST)\n",
    "        batchFeeder = self.nextBatch(xTrain.shape[0], self.miniBatchSize)\n",
    "    \n",
    "        numIter = 0\n",
    "        changedEpoch = 1\n",
    "        loss = np.zeros(10, dtype=np.float_) # saves the 10 previous checkpoint losses\n",
    "        stop = False\n",
    "        \n",
    "        print('Start training hidden nodes ' + str(self.numHiddenNodes))\n",
    "        print('Learning rate is {}'.format(self.learningRate))\n",
    "    \n",
    "        while stop == False:\n",
    "            # get batch\n",
    "            firstInd, lastInd, numEpoch = next(batchFeeder)\n",
    "            board = xTrain[firstInd:lastInd]\n",
    "            result = resultOneHot.fit_transform(yTrain[firstInd:lastInd])\n",
    "            \n",
    "            # calc and save loss every 50 iterations\n",
    "            if numIter % 50 == 0:\n",
    "                i = (numIter // 50) % 10\n",
    "                loss[i] = self.loss(board, result)\n",
    "                #print('Iteration: {0} Loss: {1}'.format(numIter, loss[i]))\n",
    "                self.saveIter(logFile, fileNum, numEpoch, numIter, loss[i])\n",
    "        \n",
    "            # calc and apply gradients\n",
    "            DJoutW, DJoutB, DJhidW, DJhidB = self.calcGradients(board, result)\n",
    "            self.applyGradients(DJoutW, DJoutB, DJhidW, DJhidB)\n",
    "        \n",
    "            # update number of iterations\n",
    "            numIter = numIter + 1\n",
    "        \n",
    "            # check if max number of iterations or epochs\n",
    "            if numIter > self.maxIter or numEpoch > self.maxEpochs:\n",
    "                stop = True\n",
    "                self.saveIter(logFile, fileNum, numEpoch - 1, numIter - 1, loss[i])\n",
    "                print('Stopped training at {} iterations and {} epochs for hidden nodes {}'.format(\n",
    "                    numIter, numEpoch, self.numHiddenNodes))\n",
    "                \n",
    "            # if decay is true change learning rate at epoch\n",
    "            if self.decay == True and numEpoch - changedEpoch > 10:\n",
    "                changedEpoch = numEpoch\n",
    "                self.learningRate = self.learningRate / 10\n",
    "                print('Epoch {}. Changed learning rate to {}'.format(numEpoch, self.learningRate))\n",
    "\n",
    "    \n",
    "    def nextBatch(self, totObs, batchSize):\n",
    "        # initialize\n",
    "        numBatches = totObs // batchSize\n",
    "        tail = totObs % batchSize\n",
    "        batch = 0\n",
    "        epoch = 0\n",
    "    \n",
    "        # generator\n",
    "        while True:\n",
    "            batch = (batch + 1) % numBatches\n",
    "            if batch == 1:\n",
    "                firstInd = 0\n",
    "                lastInd = batchSize\n",
    "                epoch = epoch + 1\n",
    "            elif batch == 0:\n",
    "                firstInd = lastInd\n",
    "                lastInd = lastInd + batchSize + tail\n",
    "            else:\n",
    "                firstInd = lastInd\n",
    "                lastInd = lastInd + batchSize\n",
    "        \n",
    "            yield firstInd, lastInd, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net of Full Data\n",
    "\n",
    "Based on sample data results, it appears that 100 is a reasonable number of hidden nodes for 294,428 total board positions in the data (of which 261,013 are unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '../data/clean'\n",
    "xTrainFiles = [os.path.join(dataDir, 'xTrain' + str(i) + '.npz') for i in range(4)]\n",
    "yTrainFiles = [os.path.join(dataDir, 'yTrain' + str(i) + '.npz') for i in range(4)]\n",
    "\n",
    "# xTrain = sp.sparse.load_npz(xTrainFiles[0])\n",
    "# yTrain = np.load(yTrainFiles[0])['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "logFile = '../log/model_full_log.txt'\n",
    "modelFile = '../model/model_full_weights.npz'\n",
    "\n",
    "# def __init__(self, numInputNodes, numHiddenNodes, numOutputNodes, maxIter, maxEpochs):\n",
    "boardFunc = BoardFunction(BOARD_LENGTH, 32000, 3, 200, 1)\n",
    "boardFunc.initWeights()\n",
    "\n",
    "# train on data chunk\n",
    "startTime = time.time()\n",
    "boardFunc.train(xTrain, yTrain, logFile, 0)\n",
    "trainTime = time.time() - startTime\n",
    "print(trainTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32000 hidden nodes, 100 iterations through each chunk\n",
    "# baseline training time\n",
    "# load through data files and train\n",
    "trainTime = 0\n",
    "for fx, fy, fileNum in zip(xTrainFiles, yTrainFiles, range(4)):\n",
    "    # load data file\n",
    "    xTrain = sp.sparse.load_npz(fx)\n",
    "    yTrain = np.load(fy)['arr_0']\n",
    "    \n",
    "    # train on data chunk\n",
    "    print('File ' + str(fileNum))\n",
    "    startTime = time.time()\n",
    "    boardFunc.train(xTrain, yTrain, logFile, fileNum)\n",
    "    trainTime = trainTime + time.time() - startTime\n",
    "    # save parameters\n",
    "    # boardFunc.saveWeights(modelFile)\n",
    "    # change learning rate\n",
    "    # boardFunc.learningRate = boardFunc.learningRate / 10\n",
    "    print()\n",
    "print(trainTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNET Implementation\n",
    "\n",
    "Gloun tutorial at http://gluon.mxnet.io/chapter03_deep-neural-networks/mlp-gluon.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model context\n",
    "dataCtx = mx.cpu()\n",
    "modelCtx = mx.cpu()\n",
    "\n",
    "# architecture\n",
    "numInputs = BOARD_LENGTH\n",
    "numHidden = 32000\n",
    "numOutputs = 3\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    dense0 = net.add(gluon.nn.Dense(numHidden, activation = 'relu'))\n",
    "    dense1 = net.add(gluon.nn.Dense(numOutputs))\n",
    "\n",
    "# parameter initialization\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=0.1), ctx=modelCtx)\n",
    "\n",
    "# softmax cross-entropy loss\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label = False)\n",
    "\n",
    "# optimizer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '../data/clean'\n",
    "xTrainFiles = [os.path.join(dataDir, 'xTrain' + str(i) + '.npz') for i in range(4)]\n",
    "yTrainFiles = [os.path.join(dataDir, 'yTrain' + str(i) + '.npz') for i in range(4)]\n",
    "\n",
    "# training loop\n",
    "\n",
    "logFile = '../log/model_full_log.txt'\n",
    "modelFile = '../model/model_full_weights'\n",
    "\n",
    "epochs = 4\n",
    "iterNum = 0\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    for fx, fy, fileNum in zip(xTrainFiles, yTrainFiles, range(4)):\n",
    "        # load data file\n",
    "        print('File ' + str(fileNum))\n",
    "        print('Loading file ' + str(fileNum))\n",
    "        xTrain = sp.sparse.load_npz(fx)\n",
    "        yTrain = np.load(fy)['arr_0']\n",
    "        print('Done loading file')\n",
    "\n",
    "        # Create batch iterator\n",
    "        # convert xTrain and yTrain to mxnet formats\n",
    "        # scipy csr format to mxnet csr format\n",
    "        print('Formating data for MXnet')\n",
    "        xTrainMX = nd.sparse.csr_matrix((xTrain.data, xTrain.indices, xTrain.indptr), dtype = 'float32')\n",
    "        yTrainMX = 1 - nd.array(np.squeeze(yTrain), dtype = 'float32') # white win = 0, draw = 1, black win = 2\n",
    "\n",
    "        trainData = io.NDArrayIter(data = xTrainMX, label = nd.one_hot(yTrainMX, depth = 3),\n",
    "                                    batch_size = 512, last_batch_handle = 'discard')\n",
    "        # trainData = io.NDArrayIter(data = xTrainMX, label = np.squeeze(yTrain).astype(np.float32),\n",
    "        #                            batch_size = 512, last_batch_handle = 'discard')\n",
    "        print('Done formating data')\n",
    "\n",
    "        print('Start training')\n",
    "        startTime = time.time()\n",
    "        for batch in trainData:\n",
    "            X = batch.data[0].as_in_context(modelCtx)\n",
    "            y = batch.label[0].as_in_context(modelCtx)\n",
    "            with autograd.record():\n",
    "                output = net(X)\n",
    "                loss = softmax_cross_entropy(output, y)\n",
    "            loss.backward()\n",
    "            trainer.step(512)\n",
    "            l = nd.sum(loss).asscalar()\n",
    "            if iterNum % 100 == 0:\n",
    "                line = str(fileNum) + ',' + str(epoch) + ',' + str(iterNum) + ',' + str(l) + '\\n'\n",
    "                with open(logFile, mode = 'a') as f:\n",
    "                    f.write(line)\n",
    "            if iterNum % 20000 == 0:\n",
    "                'checkpoint parameters'\n",
    "                net.save_params(modelFile)\n",
    "            iterNum = iterNum + 1\n",
    "        \n",
    "        endTime = time.time()\n",
    "        print('Done training file in ' + str(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model params\n",
    "modelFile = '../model/model_full_weights'\n",
    "net.load_params('../model/model_full_weights', modelCtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for param in params.values():\n",
    "    #print(param.name, param.data())\n",
    "    weights.append(param.data().asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../model/model_full_weights.npz',\n",
    "                            hiddenWeights = weights[0],\n",
    "                            hiddenBiases = weights[1],\n",
    "                            outputWeights = weights[2],\n",
    "                            outputBiases = weights[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_examples = 60000\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()\n",
    "\n",
    "batch_size = 64\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_examples = 60000\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                              batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]\n",
    "\n",
    "net = gluon.nn.Dense(num_outputs)\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=model_ctx)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "\n",
    "evaluate_accuracy(test_data, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mxnet_p36]",
   "language": "python",
   "name": "conda-env-mxnet_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
