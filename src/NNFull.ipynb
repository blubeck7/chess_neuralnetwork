{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load packages\n",
    "import sys\n",
    "sys.path.append(\"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/\")\n",
    "import boto3\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack as vstack\n",
    "import chess, chess.pgn\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "BOARD_LENGTH = 768 #chess board is 8 x 8 and 12 different pieces\n",
    "\n",
    "## Vector representation of chess board\n",
    "# v = 1 x BOARD_LENGTH\n",
    "#\n",
    "# White = Upper Case, black = lower case\n",
    "# Piece order: P, N, B, R, Q, K, p, n, b, r, q, k\n",
    "# Board order:\n",
    "#    Start at square a1. Move across the columns to square h1.\n",
    "#    Then go up a row to square a2. Move across the columns to square h2.\n",
    "#    Repeat until square h8\n",
    "#    i.e. 0 - a1, 1 - b1, ..., 7 - h1, 8 - a2, ..., 63 - h8\n",
    "#\n",
    "# Board vector indices: \n",
    "# v[0,...,63] = P, v[64,...,127] = N, ..., v[704,...,767] = k\n",
    "# v[0,...,7] = row 1; v[8,...,15] = row 2, ..., v[56,...,63] = row 8\n",
    "# v[0] = col a, v[1] = col b, ..., v[7] = col h\n",
    "\n",
    "PIECE_OFFSETS = {'P': 0, 'N': 64, 'B': 128, 'R': 192, 'Q': 256, 'K': 320,\n",
    "                 'p': 384, 'n': 448, 'b': 512, 'r': 576, 'q': 640, 'k': 704}\n",
    "\n",
    "RESULTS_DICT = {'1-0': 1,'1/2-1/2': 0,'0-1': -1}\n",
    "RESULTS_LIST = [1, 0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "\n",
    "In this section, we build, train and test a feedforward neural network that calculates the probability a board position results in a win for white. We first use a small sample of games to determine a reasonable number of hidden nodes fro the neural network. The program ParseData.py converts the pgn data into binary vectors and saves the output. The program PrepareTrainTest.py takes the binary vectors and creates four training datasets and one testing dataset. We split the data up so that the data could be processed by different cores using mini-batch parallelism. We compare the time to train the neural network of our implementation of mini-batch parallelism versus none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Class\n",
    "\n",
    "The code uses sparse matrices to siginificantly decrease the amount of memory and instructions needed on a CPU to train the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoardFunction:\n",
    "    \n",
    "    def __init__(self, numInputNodes, numHiddenNodes, numOutputNodes, maxIter, maxEpochs):\n",
    "        # layers\n",
    "        self.numInputNodes = numInputNodes\n",
    "        self.numHiddenNodes = numHiddenNodes\n",
    "        self.numOutputNodes = numOutputNodes\n",
    "        \n",
    "        # weight matrices\n",
    "        self.hiddenWeights = np.empty((self.numInputNodes, self.numHiddenNodes), dtype = np.float_)\n",
    "        self.hiddenBiases = np.empty((1, self.numHiddenNodes), dtype = np.float_)\n",
    "        self.outputWeights = np.empty((self.numHiddenNodes, self.numOutputNodes), dtype = np.float_)\n",
    "        self.outputBiases = np.empty((1, self.numOutputNodes), dtype = np.float_)\n",
    "        \n",
    "        # learning parameters\n",
    "        self.learningRate = 0.1\n",
    "        self.minRate = 0.00001\n",
    "        self.miniBatchSize = 512\n",
    "        self.maxIter = maxIter\n",
    "        self.maxEpochs = maxEpochs\n",
    "        self.minTol = 10**(-7)\n",
    "        self.decay = True # if true decreses the learning rate if loss plateaus\n",
    "        self.logPath = '../log'\n",
    "        \n",
    "    def initWeights(self, seed = None):\n",
    "        '''\n",
    "        Randomly initializes the weight matrices\n",
    "        '''\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        self.hiddenWeights = np.random.normal(loc = 0.1, size = self.hiddenWeights.shape)\n",
    "        self.hiddenBiases = np.random.normal(loc = 0.1, size = self.hiddenBiases.shape)\n",
    "        self.outputWeights = np.random.normal(loc = 0.1, size = self.outputWeights.shape)\n",
    "        self.outputBiases = np.random.normal(loc = 0.1, size = self.outputBiases.shape)\n",
    "        \n",
    "    def _relu(self, X):\n",
    "        '''\n",
    "        X - matrix\n",
    "        \n",
    "        returns element wise max of X and zero\n",
    "        '''\n",
    "        \n",
    "        return(np.maximum(X,0))\n",
    "    \n",
    "    def _softmax(self, X):\n",
    "        shiftX = X - np.amax(X, axis = 1, keepdims = True)\n",
    "        exps = np.exp(shiftX)\n",
    "        sums = np.sum(exps, axis = 1, keepdims = True)\n",
    "        \n",
    "        return(exps / sums)\n",
    "    \n",
    "    def predict(self, board):\n",
    "        '''\n",
    "        board - csr matrix: sparse row matrix of encoded board positions\n",
    "    \n",
    "        returns probs - numpy array: a matrix containing the probability of a win, draw or loss\n",
    "        '''\n",
    "        \n",
    "        numBoards = board.shape[0]\n",
    "        \n",
    "        hiddenWeights = board.dot(self.hiddenWeights)\n",
    "        hiddenBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.hiddenBiases)\n",
    "        hiddenIn = hiddenWeights + hiddenBiases\n",
    "        hiddenOut = self._relu(hiddenIn) #rectified linear element-wise max with zero\n",
    "        \n",
    "        outputWeights = hiddenOut.dot(self.outputWeights)\n",
    "        outputBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.outputBiases)\n",
    "        outputIn = outputWeights + outputBiases\n",
    "        outputOut = self._softmax(outputIn)\n",
    "        \n",
    "        minProb = np.finfo(np.float64).tiny # avoid numerical issues with zero probs\n",
    "        \n",
    "        return(np.maximum(outputOut, minProb))\n",
    "    \n",
    "    def loss(self, board, result):\n",
    "        '''\n",
    "        board - csr matrix: sparse row matrix of encoded board positions\n",
    "        result - 1d array: one hot enconding of result\n",
    "        \n",
    "        returns the cross entropy (multinomial log-likelihood) for the sample\n",
    "        '''\n",
    "        \n",
    "        probs = self.predict(board)\n",
    "        aveLogLikelihood = -np.sum(result * np.log(probs)) / board.shape[0]\n",
    "        \n",
    "        return(aveLogLikelihood)\n",
    "    \n",
    "    def calcGradients(self, board, result):\n",
    "        '''\n",
    "        board - csr matrix: sparse row matrix of encoded board positions\n",
    "        result - 1d array: one hot enconding of result\n",
    "        \n",
    "        J = cross entropy loss function\n",
    "        '''\n",
    "        \n",
    "        numBoards = board.shape[0]\n",
    "        \n",
    "        # feed forward\n",
    "        hiddenWeights = board.dot(self.hiddenWeights)\n",
    "        hiddenBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.hiddenBiases)\n",
    "        hiddenIn = hiddenWeights + hiddenBiases\n",
    "        hiddenOut = self._relu(hiddenIn) #rectified linear element-wise max with zero\n",
    "        \n",
    "        outputWeights = hiddenOut.dot(self.outputWeights)\n",
    "        outputBiases = np.outer(np.ones((numBoards, 1), dtype = np.float_), self.outputBiases)\n",
    "        outputIn = outputWeights + outputBiases\n",
    "        outputOut = self._softmax(outputIn)\n",
    "        \n",
    "        # compute gradients\n",
    "        d1 = outputOut - result\n",
    "        d2 = d1.dot(self.outputWeights.transpose()) * np.sign(hiddenOut)\n",
    "        \n",
    "        # D J(outputWeights)\n",
    "        DJoutW = hiddenOut.transpose().dot(d1) / numBoards\n",
    "        \n",
    "        # D J(outputBiases)\n",
    "        DJoutB = np.sum(d1.dot(np.eye(result.shape[1])), axis = 0) / numBoards\n",
    "        \n",
    "        # D J(hiddenWeights)\n",
    "        DJhidW = board.transpose().dot(d2) / numBoards\n",
    "        \n",
    "        # D J(hiddenBiases)\n",
    "        DJhidB = np.sum(d2, axis = 0) / numBoards\n",
    "        \n",
    "        return(DJoutW, DJoutB, DJhidW, DJhidB)\n",
    "    \n",
    "    def saveIter(self, fileName, fileNum, numEpoch, numIter, loss):\n",
    "        line = str(fileNum) + ',' + str(numEpoch) + ',' + str(numIter) + ',' + str(loss) + '\\n'\n",
    "        with open(fileName, mode ='a') as f:\n",
    "            f.write(line)\n",
    "    \n",
    "    def saveWeights(self, fileName):\n",
    "        print('Saving weights to ' + fileName)\n",
    "        np.savez_compressed(fileName,\n",
    "                            hiddenWeights = self.hiddenWeights,\n",
    "                            hiddenBiases = self.hiddenBiases,\n",
    "                            outputWeights = self.outputWeights,\n",
    "                            outputBiases = self.outputBiases)\n",
    "        print('Done saving weights')\n",
    "        \n",
    "    def applyGradients(self, DJoutW, DJoutB, DJhidW, DJhidB):\n",
    "        '''\n",
    "        \n",
    "        updates each weight matrix by subtracting off the learning rate times the gradient\n",
    "        '''\n",
    "        \n",
    "        self.hiddenWeights = self.hiddenWeights - self.learningRate * DJhidW\n",
    "        self.hiddenBiases = self.hiddenBiases - self.learningRate * DJhidB\n",
    "        self.outputWeights = self.outputWeights - self.learningRate * DJoutW\n",
    "        self.outputBiases = self.outputBiases - self.learningRate * DJoutB\n",
    "    \n",
    "    def train(self, xTrain, yTrain, logFile, fileNum):\n",
    "        \n",
    "        # setup training\n",
    "        resultOneHot = MultiLabelBinarizer(classes = RESULTS_LIST)\n",
    "        batchFeeder = self.nextBatch(xTrain.shape[0], self.miniBatchSize)\n",
    "    \n",
    "        numIter = 0\n",
    "        changedEpoch = 1\n",
    "        loss = np.zeros(10, dtype=np.float_) # saves the 10 previous checkpoint losses\n",
    "        stop = False\n",
    "        \n",
    "        print('Start training hidden nodes ' + str(self.numHiddenNodes))\n",
    "        print('Learning rate is {}'.format(self.learningRate))\n",
    "    \n",
    "        while stop == False:\n",
    "            # get batch\n",
    "            firstInd, lastInd, numEpoch = next(batchFeeder)\n",
    "            board = xTrain[firstInd:lastInd]\n",
    "            result = resultOneHot.fit_transform(yTrain[firstInd:lastInd])\n",
    "            \n",
    "            # calc and save loss every 50 iterations\n",
    "            if numIter % 50 == 0:\n",
    "                i = (numIter // 50) % 10\n",
    "                loss[i] = self.loss(board, result)\n",
    "                #print('Iteration: {0} Loss: {1}'.format(numIter, loss[i]))\n",
    "                self.saveIter(logFile, fileNum, numEpoch, numIter, loss[i])\n",
    "            \n",
    "            # save parameters\n",
    "            if numIter % 10000 == 0:\n",
    "                # save parameters\n",
    "                print('Saving weights learned so far')\n",
    "                boardFunc.saveWeights(modelFile)\n",
    "        \n",
    "            # calc and apply gradients\n",
    "            DJoutW, DJoutB, DJhidW, DJhidB = self.calcGradients(board, result)\n",
    "            self.applyGradients(DJoutW, DJoutB, DJhidW, DJhidB)\n",
    "        \n",
    "            # update number of iterations\n",
    "            numIter = numIter + 1\n",
    "        \n",
    "            # check if max number of iterations or epochs\n",
    "            if numIter > self.maxIter or numEpoch > self.maxEpochs:\n",
    "                stop = True\n",
    "                self.saveIter(logFile, fileNum, numEpoch - 1, numIter - 1, loss[i])\n",
    "                print('Stopped training at {} iterations and {} epochs for hidden nodes {}'.format(\n",
    "                    numIter, numEpoch, self.numHiddenNodes))\n",
    "                \n",
    "            # if decay is true change learning rate at epoch\n",
    "            if self.decay == True and numEpoch - changedEpoch > 10:\n",
    "                changedEpoch = numEpoch\n",
    "                self.learningRate = self.learningRate / 10\n",
    "                print('Epoch {}. Changed learning rate to {}'.format(numEpoch, self.learningRate))\n",
    "\n",
    "    \n",
    "    def nextBatch(self, totObs, batchSize):\n",
    "        # initialize\n",
    "        numBatches = totObs // batchSize\n",
    "        tail = totObs % batchSize\n",
    "        batch = 0\n",
    "        epoch = 0\n",
    "    \n",
    "        # generator\n",
    "        while True:\n",
    "            batch = (batch + 1) % numBatches\n",
    "            if batch == 1:\n",
    "                firstInd = 0\n",
    "                lastInd = batchSize\n",
    "                epoch = epoch + 1\n",
    "            elif batch == 0:\n",
    "                firstInd = lastInd\n",
    "                lastInd = lastInd + batchSize + tail\n",
    "            else:\n",
    "                firstInd = lastInd\n",
    "                lastInd = lastInd + batchSize\n",
    "        \n",
    "            yield firstInd, lastInd, epoch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Built on Sample Data\n",
    "\n",
    "We build a neural net on a sample of data in order to determine a resonable number of hidden nodes to use to\n",
    "train the full model. We use a sample of data so that the training times are short enough that the model can be iterated on as a function of number of hidden nodes on a laptop. First, we write the code to implement a feed forward neural network with one hidden layer. The hidden layer uses the rectified linear function, and the output layer uses the softmax function. Cross-entropy (multinomial likelihood) is used as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load sample data\n",
    "dataDir = '../test/clean'\n",
    "xTrainFiles = [os.path.join(dataDir, 'xTrain'+str(i)+'.npz') for i in range(os.cpu_count())]\n",
    "yTrainFiles = [os.path.join(dataDir, 'yTrain'+str(i)+'.npz') for i in range(os.cpu_count())]\n",
    "xTrain_ = []\n",
    "yTrain_ = []\n",
    "\n",
    "for fx, fy in zip(xTrainFiles,yTrainFiles):\n",
    "    xTrain_.append(scipy.sparse.load_npz(fx))\n",
    "    yTrain_.append(np.load(fy)['arr_0'])\n",
    "    \n",
    "xTrain1 = scipy.sparse.vstack(xTrain_)\n",
    "yTrain1 = np.concatenate(yTrain_)\n",
    "\n",
    "xTest1 = scipy.sparse.load_npz(os.path.join(dataDir, 'xTest.npz'))\n",
    "yTest1 = np.load(os.path.join(dataDir, 'yTest.npz'))['arr_0']\n",
    "\n",
    "del [xTrain_, yTrain_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define worker function for bias-variance graph\n",
    "def evalNumHiddenNodes(numHiddenNodes):\n",
    "    '''\n",
    "    numHiddenNodes - int: number of hidden nodes\n",
    "    \n",
    "    returns tuple of training loss and testing loss\n",
    "    '''\n",
    "    \n",
    "    resultOneHot = MultiLabelBinarizer(classes = RESULTS_LIST)\n",
    "    trainingLoss = 0\n",
    "    testingLoss = 0\n",
    "    \n",
    "    print('Hidden Nodes {}'.format(numHiddenNodes))\n",
    "    \n",
    "    for i in range(4):\n",
    "        print('Hidden Nodes {} round {}'.format(numHiddenNodes, i))\n",
    "        xTrain_, xTest_, yTrain_, yTest_ = train_test_split(vstack((xTrain,xTest)), np.vstack((yTrain,yTest)),\n",
    "                                                            train_size = 0.8, random_state = i + 1)\n",
    "        f = BoardFunction(BOARD_LENGTH, numHiddenNodes, 3, 200000, 20)\n",
    "        f.initWeights(i + 1) #set seed for reproducibility\n",
    "        f.train(xTrain_, yTrain_)\n",
    "        trainingLoss = trainingLoss + f.loss(xTrain_, resultOneHot.fit_transform(yTrain_)) / 4\n",
    "        testingLoss = testingLoss + f.loss(xTest_, resultOneHot.fit_transform(yTest_)) / 4\n",
    "    \n",
    "    return(trainingLoss, testingLoss)\n",
    "\n",
    "def worker(taskQueue, outQueue):\n",
    "    while not taskQueue.empty():\n",
    "        n = taskQueue.get()\n",
    "        trainingLoss, testingLoss = evalNumHiddenNodes(n)\n",
    "        outQueue.put((n, trainingLoss, testingLoss))\n",
    "\n",
    "def main():\n",
    "    # set number of processes to number of cores\n",
    "    numProcesses = os.cpu_count()\n",
    "    \n",
    "    # create queues\n",
    "    taskQueue = mp.Queue()\n",
    "    outQueue = mp.Queue()\n",
    "\n",
    "    # create task queue with range of hidden nodes to evaluate\n",
    "    # hiddenNodes = [2, 3, 5] + [i for i in range(10,100,10)] + [i for i in range(100,1100,100)]\n",
    "    hiddenNodes = [2000]\n",
    "    # submit tasks\n",
    "    for i in hiddenNodes:\n",
    "        taskQueue.put(i)\n",
    "        \n",
    "    # Start worker processes\n",
    "    workers = []\n",
    "    for i in range(numProcesses):\n",
    "        workers.append(mp.Process(target=worker, args=(taskQueue, outQueue)))\n",
    "        workers[i].daemon = True\n",
    "        workers[i].start()\n",
    "    \n",
    "    # Block until all items in the queue have been retrieved and processed.\n",
    "    for w in workers:\n",
    "        w.join()\n",
    "        \n",
    "    # Get and print results\n",
    "    print('Results:')\n",
    "    for i in range(len(hiddenNodes)):\n",
    "        print('\\t', outQueue.get())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net of Full Data\n",
    "\n",
    "Based on data below, it appears that 100 is a reasonable number of hidden nodes for 294,428 total board positions in the data (of which 261,013 are unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDir = '../data/clean'\n",
    "xTrainFiles = [os.path.join(dataDir, 'xTrain' + str(i) + '.npz') for i in range(4)]\n",
    "yTrainFiles = [os.path.join(dataDir, 'yTrain' + str(i) + '.npz') for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0\n",
      "Start training hidden nodes 32000\n",
      "Learning rate is 0.1\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Stopped training at 46992 iterations and 2 epochs for hidden nodes 32000\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "\n",
      "File 1\n",
      "Start training hidden nodes 32000\n",
      "Learning rate is 0.1\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Stopped training at 46992 iterations and 2 epochs for hidden nodes 32000\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "\n",
      "File 2\n",
      "Start training hidden nodes 32000\n",
      "Learning rate is 0.1\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Stopped training at 46992 iterations and 2 epochs for hidden nodes 32000\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "\n",
      "File 3\n",
      "Start training hidden nodes 32000\n",
      "Learning rate is 0.1\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Saving weights learned so far\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "Stopped training at 46992 iterations and 2 epochs for hidden nodes 32000\n",
      "Saving weights to ../model/model_full_weights.npz\n",
      "Done saving weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "logFile = '../log/model_full_log.txt'\n",
    "modelFile = '../model/model_full_weights.npz'\n",
    "boardFunc = BoardFunction(BOARD_LENGTH, 32000, 3, 200000, 1)\n",
    "boardFunc.initWeights()\n",
    "\n",
    "# load through data files and train\n",
    "for fx, fy, fileNum in zip(xTrainFiles, yTrainFiles, range(4)):\n",
    "    # load data file\n",
    "    xTrain = scipy.sparse.load_npz(fx)\n",
    "    yTrain = np.load(fy)['arr_0']\n",
    "    \n",
    "    # train on data chunk\n",
    "    print('File ' + str(fileNum))\n",
    "    boardFunc.train(xTrain, yTrain, logFile, fileNum)\n",
    "    # save parameters\n",
    "    boardFunc.saveWeights(modelFile)\n",
    "    # change learning rate\n",
    "    boardFunc.learningRate = boardFunc.learningRate\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82741939, -0.66027762,  0.7655464 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boardFunc.outputBiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# memory usage in GiB\n",
    "dataGiB = xTrain.nnz * xTrain.dtype.itemsize / (1024**3)\n",
    "colGiB = xTrain.indices.shape[0] * xTrain.indices.dtype.itemsize / (1024**3)\n",
    "rowGiB = xTrain.indptr.shape[0] * xTrain.indptr.dtype.itemsize / (1024**3)\n",
    "print('Boards total GiB: ' + str(dataGiB + colGiB + rowGiB))\n",
    "\n",
    "dataGiB = yTrain.shape[0] * yTrain.dtype.itemsize / (1024**3)\n",
    "print('Results total GiB: ' + str(dataGiB))\n",
    "\n",
    "print('Number of Board Positions: ' + str(xTrain.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for Report\n",
    "\n",
    "From Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. https://arxiv.org/abs/1609.04836 :\n",
    "\n",
    "The stochastic gradient descent method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, usually 32--512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of Training on Small sample\n",
    "\n",
    "Hidden Nodes | Training Loss | Testing Loss\n",
    "\n",
    "(2, 1.0100619674035751, 1.0091796152110315)  \n",
    "(3, 1.007482207571287, 1.0066383559043466)  \n",
    "(5, 1.0055554590398994, 1.005023531581839)  \n",
    "(10, 1.0025096545121017, 1.0014980079650992)  \n",
    "(20, 0.99807602689486419, 0.99720644407485393)  \n",
    "(30, 0.99399420182901932, 0.99333884848858678)  \n",
    "(40, 0.99328607731974783, 0.99274053552888364)  \n",
    "(50, 0.98944021407430505, 0.98889769712408171)  \n",
    "(60, 0.9863994075288971, 0.98619975531255344)  \n",
    "(70, 0.9840429241801889, 0.98366685082558569)  \n",
    "(80, 0.98182486550393266, 0.98176263405153508)  \n",
    "(90, 0.98028274459491105, 0.980031350473068)  \n",
    "(100, 0.9799195740106964, 0.98035206647664563)  \n",
    "(200, 0.96067535070753229, 0.96146881491348635)  \n",
    "(300, 0.94619056342381636, 0.94883739830555913)  \n",
    "(400, 0.93211902538475155, 0.93581461710661318)  \n",
    "(500, 0.92147218306629264, 0.92625700867560345)  \n",
    "(600, 0.91413918085285739, 0.91944739566420786)  \n",
    "(700, 0.90181913873124897, 0.90894246695116876)  \n",
    "(800, 0.89781815061434322, 0.90720724133753783)  \n",
    "(900, 0.90179592770302441, 0.91455607692081964)  \n",
    "(1000, 0.90999905019713589, 0.92927055677816228)  \n",
    "(1100, 0.91730496589972221, 0.94067571972455488)  \n",
    "(1200, 0.93264025467090339, 0.96354963998689114)  \n",
    "(1300, 1.0002378034267088, 1.0420546608713139)  \n",
    "(1400, 1.0517057482093932, 1.1026789728980475)  \n",
    "(1500, 1.1478033939379304, 1.221898995717857)  \n",
    "(1600, 1.2890920718141325, 1.3911260895054278)  \n",
    "(1700, 1.3804783955461297, 1.5090146237581545)  \n",
    "(1800, 1.4683900170764266, 1.6101375997534371)  \n",
    "(1900, 1.6144862319988333, 1.7925763627477731)  \n",
    "(2000, 1.7326184003683225, 1.9542765976494103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertBoardToVec(board):\n",
    "    '''convertBoardToVec(board object) -> array\n",
    "        \n",
    "        board object = object of Board Class from chess,\n",
    "        array = 1d np array of length BOARD_LENGTH\n",
    "        \n",
    "    This function loops converts a board to its corresponding vector representation\n",
    "    '''\n",
    "    \n",
    "    v = np.zeros(BOARD_LENGTH, dtype = np.int8)\n",
    "\n",
    "    pieces = board.piece_map()\n",
    "    for sq in pieces:\n",
    "        piece = pieces[sq]\n",
    "        ind = PIECE_OFFSETS[piece.symbol()] + sq\n",
    "        v[ind] = 1\n",
    "        \n",
    "    return(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = BoardFunction(BOARD_LENGTH, 10, 3, 200000, 20)\n",
    "f.initWeights(1) #set seed for reproducibility\n",
    "f.train(xTrain, yTrain)\n",
    "resultOneHot = MultiLabelBinarizer(classes = RESULTS_LIST)\n",
    "trainingLoss = f.loss(xTrain, resultOneHot.fit_transform(yTrain))\n",
    "testingLoss = f.loss(xTest, resultOneHot.fit_transform(yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trainingLoss, testingLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer parameters\n",
    "numInputNodes = BOARD_LENGTH\n",
    "numHiddenNodes = 10\n",
    "numOutputNodes = 3\n",
    "\n",
    "# input and output placeholders\n",
    "#x = tf.sparse_placeholder(tf.float64, shape = [None, numInputNodes])\n",
    "x = tf.placeholder(tf.float64, shape = [None, numInputNodes])\n",
    "y = tf.placeholder(tf.float64, shape = [None, numOutputNodes])\n",
    "\n",
    "# layer weights and biases\n",
    "hiddenWeights = tf.Variable(f.hiddenWeights)\n",
    "hiddenBiases = tf.Variable(f.hiddenBiases)\n",
    "outputWeights = tf.Variable(f.outputWeights)\n",
    "outputBiases = tf.Variable(f.outputBiases)\n",
    "\n",
    "# computations\n",
    "# hidden = tf.nn.relu(tf.add(tf.sparse_tensor_dense_matmul(x, hiddenWeights), hiddenBiases))\n",
    "hidden = tf.nn.relu(tf.add(tf.matmul(x, hiddenWeights), hiddenBiases))\n",
    "output = tf.add(tf.matmul(hidden, outputWeights), outputBiases)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = output))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    currCost = sess.run([cost], feed_dict = {x: xTest.toarray(), y: resultOneHot.fit_transform(yTest)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "currCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
